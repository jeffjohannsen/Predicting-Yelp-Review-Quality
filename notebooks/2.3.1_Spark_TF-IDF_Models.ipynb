{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "primary-brunswick",
   "metadata": {},
   "source": [
    "# Text Processing - Yelp 2021 - Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-winner",
   "metadata": {},
   "source": [
    "This notebook covers:\n",
    "* Tf-Idf Text Vectorization\n",
    "* Naive Bayes Predictions\n",
    "* Support Vector Machine Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-theology",
   "metadata": {},
   "source": [
    "## Imports and Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "orange-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Basic PySpark\n",
    "import pyspark as ps\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "# PySpark NLP\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, StringIndexer, IndexToString\n",
    "# PySpark Classification Models\n",
    "from pyspark.ml.classification import NaiveBayes, LinearSVC\n",
    "# PySpark Model Evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-obligation",
   "metadata": {},
   "source": [
    "## Set Up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broke-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (ps.sql.SparkSession.builder\n",
    ".appName(\"Spark NLP\")\n",
    ".master(\"local[3]\")\n",
    ".config(\"spark.driver.memory\",\"16G\")\n",
    ".config(\"spark.driver.maxResultSize\", \"0\")\n",
    ".config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    ".config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0\")\n",
    ".config(\"spark.driver.extraClassPath\", \"/home/jovyan/postgresql-42.2.20.jar\")\n",
    ".getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-clone",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-marshall",
   "metadata": {},
   "source": [
    "### Connecting To Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electoral-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": None,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "db_url = 'jdbc:postgresql://None/yelp_2021_db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "naval-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = spark.read.jdbc(url=db_url,table='(SELECT review_id, review_text, target_ufc_bool FROM text_data_train) AS tmp_train',properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "split-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.read.jdbc(url=db_url,table='(SELECT review_id, review_text, target_ufc_bool FROM text_data_test) AS tmp_test',properties=db_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disabled-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.createOrReplaceTempView(\"train\")\n",
    "test.createOrReplaceTempView(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-criminal",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fifteen-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- target_ufc_bool: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "becoming-columbus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n",
      "|           review_id|         review_text|target_ufc_bool|\n",
      "+--------------------+--------------------+---------------+\n",
      "|Y5G32BbSbiMlzLCJn...|Love this place!!...|           True|\n",
      "|Y5MspuNJc4wvEN7To...|Came here last ni...|          False|\n",
      "|Y5WF0rNHJjbqn-JHp...|There must be a w...|          False|\n",
      "|Y5WikZeyRj-B3a5Qf...|The staff was ver...|           True|\n",
      "|Y5ahrqYGEhMif-_YD...|Waited forever (m...|          False|\n",
      "+--------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "continent-omaha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Records: 5523992\n",
      "Test Records: 1382379\n"
     ]
    }
   ],
   "source": [
    "print(f'Train Records: {train.count()}')\n",
    "print(f'Test Records: {test.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-cuisine",
   "metadata": {},
   "source": [
    "## Prep Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-europe",
   "metadata": {},
   "source": [
    "### Majority Class Baseline (True or Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "therapeutic-cream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-------+\n",
      "|target_ufc_bool|  count|percent|\n",
      "+---------------+-------+-------+\n",
      "|           True|2796729|  50.63|\n",
      "|          False|2727263|  49.37|\n",
      "+---------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Baseline\n",
    "train_baseline = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT target_ufc_bool,\n",
    "           COUNT(*) AS count,\n",
    "           ROUND((COUNT(*) / (SELECT COUNT(*) FROM train)) * 100, 2) AS percent\n",
    "    FROM train\n",
    "    GROUP BY target_ufc_bool\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pointed-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+-------+\n",
      "|target_ufc_bool| count|percent|\n",
      "+---------------+------+-------+\n",
      "|           True|699072|  50.57|\n",
      "|          False|683307|  49.43|\n",
      "+---------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Baseline\n",
    "test_baseline = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT target_ufc_bool,\n",
    "           COUNT(*) AS count,\n",
    "           ROUND((COUNT(*) / (SELECT COUNT(*) FROM test)) * 100, 2) AS percent\n",
    "    FROM test\n",
    "    GROUP BY target_ufc_bool\n",
    "    ORDER BY count DESC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-pakistan",
   "metadata": {},
   "source": [
    "## Text Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outdoor-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alone-penetration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = (DocumentAssembler()\n",
    "                     .setInputCol('review_text')\n",
    "                     .setOutputCol('document'))\n",
    "\n",
    "sentence = (SentenceDetector()\n",
    "            .setInputCols(['document'])\n",
    "            .setOutputCol('sentence'))\n",
    "\n",
    "tokenizer = (Tokenizer()\n",
    "             .setInputCols(['sentence'])\n",
    "             .setOutputCol('token'))\n",
    "\n",
    "normalizer = (Normalizer()\n",
    "              .setInputCols(['token'])\n",
    "              .setOutputCol('normalized')\n",
    "              .setLowercase(True))\n",
    "\n",
    "lemmatizer = (LemmatizerModel.pretrained()\n",
    "              .setInputCols(['normalized'])\n",
    "              .setOutputCol('lemma'))\n",
    "\n",
    "stopwords_cleaner = (StopWordsCleaner()\n",
    "                     .setInputCols(['lemma'])\n",
    "                     .setOutputCol('clean_lemma')\n",
    "                     .setCaseSensitive(False)\n",
    "                     .setStopWords(eng_stopwords))\n",
    "\n",
    "finisher = (Finisher()\n",
    "            .setInputCols(['clean_lemma'])\n",
    "            .setOutputCols([\"token_features\"])\n",
    "            .setOutputAsArray(True)\n",
    "            .setCleanAnnotations(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressed-suspect",
   "metadata": {},
   "source": [
    "### Class Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "possible-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_strIdx = StringIndexer(inputCol=\"target_ufc_bool\", outputCol=\"label\", stringOrderType='alphabetAsc')\n",
    "label_Idxstr = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_class\", labels=[\"False\", \"True\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-intro",
   "metadata": {},
   "source": [
    "### Text Prep Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "warming-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashTF = HashingTF(inputCol=\"token_features\", outputCol=\"tf_features\")\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"features\", minDocFreq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-forth",
   "metadata": {},
   "source": [
    "### Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "numeric-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNB_CLF = NaiveBayes(smoothing=1.0) # https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.NaiveBayes.html\n",
    "SVM_CLF = LinearSVC(standardization=False) # https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LinearSVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-profile",
   "metadata": {},
   "source": [
    "### Loading Everything to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sixth-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = (Pipeline()\n",
    "            .setStages([documentAssembler,\n",
    "                        sentence,\n",
    "                        tokenizer,\n",
    "                        normalizer,\n",
    "                        lemmatizer,\n",
    "                        stopwords_cleaner,\n",
    "                        finisher,\n",
    "                        hashTF,\n",
    "                        idf,\n",
    "                        label_strIdx,\n",
    "                        MNB_CLF,\n",
    "                        label_Idxstr\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-enterprise",
   "metadata": {},
   "source": [
    "### Fit and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "criminal-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_start = time.perf_counter()\n",
    "cls_model = pipeline.fit(train)\n",
    "fit_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "tight-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_start = time.perf_counter()\n",
    "test_pred = cls_model.transform(test)\n",
    "train_pred = cls_model.transform(train)\n",
    "transform_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-classroom",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "international-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_start = time.perf_counter()\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(test_pred, {evaluator.metricName: \"areaUnderROC\"})\n",
    "aupr = evaluator.evaluate(test_pred, {evaluator.metricName: \"areaUnderPR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hispanic-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "accuracy = multi_evaluator.evaluate(test_pred, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(test_pred, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(test_pred, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 = multi_evaluator.evaluate(test_pred, {multi_evaluator.metricName: \"f1\"})\n",
    "eval_end = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "certified-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.603\n",
      "AUC: 0.324\n",
      "AUPR: 0.398\n",
      "Precision: 0.611\n",
      "Recall: 0.603\n",
      "F1 Score: 0.598\n",
      "Fit Time: 955.46 minutes\n",
      "Transform/Predict Time: 0.97 seconds\n",
      "Eval Time: 817.27 minutes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"AUC: {auc:.3f}\")\n",
    "print(f\"AUPR: {aupr:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")\n",
    "print(f\"Fit Time: {(fit_end - fit_start)/60:.2f} minutes\")\n",
    "print(f\"Transform/Predict Time: {transform_end - transform_start:.2f} seconds\")\n",
    "print(f\"Eval Time: {(eval_end - eval_start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-volume",
   "metadata": {},
   "source": [
    "### Saving Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "concerned-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = train_pred.withColumn(\"Prob\", vector_to_array(\"probability\"))\n",
    "test_pred = test_pred.withColumn(\"Prob\", vector_to_array(\"probability\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aggressive-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred.createOrReplaceTempView(\"train_pred\")\n",
    "test_pred.createOrReplaceTempView(\"test_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "specified-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_finished = spark.sql(\"\"\"\n",
    "                            SELECT review_id,\n",
    "                                ROUND(Prob[1], 3) AS NB_tfidf_true_prob\n",
    "                            FROM train_pred\n",
    "                           \"\"\")\n",
    "\n",
    "test_finished = spark.sql(\"\"\"\n",
    "                            SELECT review_id,\n",
    "                                ROUND(Prob[1], 3) AS NB_tfidf_true_prob\n",
    "                            FROM test_pred\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "obvious-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_finished.write.jdbc(url=db_url,table='text_data_train_nm_tfidf',mode='overwrite',properties=db_properties)\n",
    "test_finished.write.jdbc(url=db_url,table='text_data_test_nm_tfidf',mode='overwrite',properties=db_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-silver",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fatal-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "thousand-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"NB_TFIDF_100k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "recognized-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model.save(f\"spark_models/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
